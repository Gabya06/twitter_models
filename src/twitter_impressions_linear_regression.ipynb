{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Models\n",
    "__ Author: Gabrielle Agrocostea__<br>\n",
    "__Last updated: March 2016__\n",
    "\n",
    "This notebook is used to predict twitter impressions based on retweets and followers.\n",
    "The model is used to initialize, clean data, train, predict and preform cross validation. \n",
    "\n",
    "### Model 1: \n",
    "The 1st model is built using all 27 properties and by remove top and bottom 10% of retweets and followers. Ridge regression is used with alpha set to 0.1, and R-squared is 0.764 using 90% training and 10% test data. Using an average error threshold  of 60% filters out properties with larger average errors such as CNN, NFL, MTV, BET and InStyle. Performing 5-fold cross validation to find the best alpha for ridge regression yields about the same score with slightly lower average errors.\n",
    "\n",
    "### Model 2: \n",
    "The 2nd model removes properties that had the highest average error (over 60%) and also performs 5-fold cross-validation and searches for best alpha for Ridge regression. The average score on the folds is higher than the 1st model (around 0.82). The largest errors are just at 50% with smaller properties, which leads me to think that maybe we need to build a model for properties such as CNN and MTV and another model for smaller properties.\n",
    "\n",
    "### Model 3: \n",
    "The 3rd model is built using only those properties with the largest errors (over 60%) and the average R-squared error is closer to 0.89, so this model outperforms the other two. While there are still some properties with large errors, some like CNN do a better job using this model than the 1st model which includes all properties. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of documentation on Model class: \n",
    "    \n",
    "The Model class initializes a model with the following attributes:\n",
    "- Twitter and user data <br>\n",
    "- Model results: a dataframe to store page_id, followers, retweets, impressions, predicted and percent differences\n",
    "- rmse score\n",
    "- n data points\n",
    "- model (linear regression)\n",
    "\n",
    "The _clean_ function:\n",
    "- Removes retweets and impressions above and below the input quantile (this is .10 by default)\n",
    "- Add weekday as a feature and drop time column\n",
    "\n",
    "The _train_ function gets randomized training and test data based on percent to train (default is 90%), fits the model and sets the score and predicted values.\n",
    "\n",
    "The _cross validate_ function takes as input alphas and the number of k-folds to run (default is 5) and performs cross-validation to find the best alpha. It returns the best alpha found based on k-fold cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    '''\n",
    "    class to build linear regression model to predict impressions based on followers & re-tweets\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize model with Twitter data with user data (not used yet)\n",
    "        Has rmse, r_score, n and model attributes\n",
    "        '''\n",
    "        self.tw_data = load_tw_data()\n",
    "        self.user_data = load_user_data()\n",
    "\n",
    "        # self.result = pd.DataFrame(columns = ['base_user','cross_user','base_total_overlap','perc_overlap'])\n",
    "        self.model_results = pd.DataFrame(columns=['page_id', 'followers', 'retweets', 'impressions', 'predicted', \\\n",
    "                                                   'perc_diff', 'err_cat'])\n",
    "        self.rmse = None\n",
    "        self.r_score = None\n",
    "        self.n = None\n",
    "        self.model = None\n",
    "\n",
    "    def clean(self, quantile):\n",
    "        '''\n",
    "        \n",
    "        :param quantile: float between 0.0 and 1.0 used to clean up top and bottom quantiles for tweets and impressions \n",
    "        '''\n",
    "        self.tw_data = self.tw_data[(self.tw_data.retweets > self.tw_data.retweets.quantile(quantile)) & (\n",
    "            self.tw_data.retweets < self.tw_data.retweets.quantile(1.0 - quantile))]\n",
    "        self.tw_data = self.tw_data[(self.tw_data.impressions > self.tw_data.impressions.quantile(quantile)) & (\n",
    "            self.tw_data.impressions < self.tw_data.impressions.quantile(1.0 - quantile))]\n",
    "        # reset new index for data\n",
    "        self.n = self.tw_data.shape[0]\n",
    "        # twitter_index = np.arange(0, self.n)\n",
    "        self.tw_data = self.tw_data.set_index(np.arange(0, self.n))\n",
    "\n",
    "        # add weekday\n",
    "        try:\n",
    "            self.tw_data['weekday'] = self.tw_data.time.map(lambda x: np.int(x.date().weekday()))\n",
    "            self.tw_data.drop('time', axis=1, inplace=True)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def _get_train_test(self, perc_train=0.9):\n",
    "        '''\n",
    "        Based on 90% training and 10% testing, set training & test data\n",
    "        :param perc_train: default is .90\n",
    "        :return: x_train, y_train, x_test, y_test data\n",
    "        '''\n",
    "        # sample for training\n",
    "        n_train = np.round(self.n * perc_train)\n",
    "        ndex_train = np.random.randint(0, self.n, int(n_train))\n",
    "\n",
    "        # # training data\n",
    "        x_train = self.tw_data.ix[ndex_train]\n",
    "        y_train = self.tw_data['impressions'][ndex_train]\n",
    "        x_train = x_train[[\"followers\", \"retweets\"]]\n",
    "\n",
    "        # # test data\n",
    "        x_test = self.tw_data.drop(ndex_train, axis=0)\n",
    "        y_test = x_test[['impressions']]\n",
    "\n",
    "        self.model_results[['page_id', 'followers', 'retweets', 'impressions']] = x_test[\n",
    "            ['page_id', 'followers', 'retweets', 'impressions']].copy()\n",
    "        x_test = x_test[[\"followers\", \"retweets\"]]\n",
    "\n",
    "        return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "    def cross_validate(self, alphas, folds):\n",
    "        '''\n",
    "        Perform k-fold cross-validation\n",
    "        :param alphas: list of alphas for parameter search \n",
    "        :param folds: how many time to perform cross-validation (default k is 5)\n",
    "        :return: best alpha found\n",
    "        '''\n",
    "        model_cv = linear_model.RidgeCV(alphas=alphas, cv=5)\n",
    "        k_fold = cross_validation.KFold(n=self.n, n_folds=folds, shuffle=True)\n",
    "        cv_scores = list()\n",
    "        cv_alphas = list()\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            # X = self.tw_data.drop(['tw_name','impressions'], axis=1)\n",
    "            X = self.tw_data.drop(['impressions'], axis=1)\n",
    "            Y = self.tw_data.impressions\n",
    "            model_cv.fit(X.ix[train], Y.ix[train])\n",
    "            model_cv.alpha_ = alphas[k]\n",
    "            cv_alphas.append(model_cv.alpha_)\n",
    "            cv_scores.append(model_cv.score(X.ix[test], Y.ix[test]))\n",
    "            print(\"[fold {0}] alpha: {1:.9f}, score: {2:.5f}\".format(k, model_cv.alpha_,\n",
    "                                                                     model_cv.score(X.ix[test], Y.ix[test])))\n",
    "        model_cv_df = pd.DataFrame({'fold': range(folds), 'alpha': cv_alphas, 'score': cv_scores})\n",
    "        print \"Best alpha's\\n\", model_cv_df.sort_values('score', ascending=False).head(10)\n",
    "        # print \"Best alpha's\\n\", model_cv_df.sort_values('score', ascending=False).head(10)\n",
    "        best_alpha = model_cv_df.sort_values('score', ascending=False).alpha.iloc[0]\n",
    "        return best_alpha\n",
    "\n",
    "    def train(self, model, perc_train, alpha):\n",
    "        '''\n",
    "        fit model on training data, set score and model results \n",
    "        :param model: linear model to set (Ridge, Lasso..)  \n",
    "        :param perc_train: percent to use for training (float between 0 & 1)\n",
    "        :param alpha: penalty value for model \n",
    "\n",
    "        '''\n",
    "        x_train, y_train, x_test, y_test = self._get_train_test(perc_train)\n",
    "        self.model = model(alpha)\n",
    "        self.model.fit(x_train, y_train)\n",
    "        self.r_score = self.model.score(x_test, y_test)\n",
    "        self.model_results.predicted = self.model.predict(x_test)\n",
    "\n",
    "    def get_coefs(self):\n",
    "        '''\n",
    "        \n",
    "        :return: model coefficients \n",
    "        '''\n",
    "        return self.model.coef_\n",
    "\n",
    "    def get_results(self):\n",
    "        tw_samples = self.user_data[['user_id', 'tw_name']].drop_duplicates()\n",
    "        tw_samples.rename(columns={'user_id': 'page_id'}, inplace=True)\n",
    "\n",
    "        diff = np.subtract(self.model_results.impressions, self.model_results.predicted)\n",
    "        self.model_results.perc_diff = abs(diff / self.model_results.impressions)\n",
    "\n",
    "        bins = np.arange(0.0, 1.0, .3)\n",
    "        large_bins = np.arange(1.0, self.model_results.perc_diff.max() + 5, 4)\n",
    "        all_bins = np.concatenate([bins, large_bins])\n",
    "\n",
    "        self.model_results['err_cat'] = pd.cut(self.model_results.perc_diff, bins=all_bins, right=True)\n",
    "        self.model_results = self.model_results.merge(tw_samples, on='page_id')\n",
    "\n",
    "        results = self.model_results.groupby(['page_id', 'tw_name', 'err_cat'])[['err_cat']].agg('count')\n",
    "        results.rename(columns={'err_cat': 'frequency'}, inplace=True)\n",
    "        results = results.reset_index()\n",
    "        results.sort_values(['frequency', 'tw_name'], ascending=False, inplace=True)\n",
    "        results['perc_total_err'] = results.groupby('err_cat')['frequency'].apply(lambda x: x / (x.sum()))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression - Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "MODEL 1 - Using all 27 properties for modeling impressions\n",
      ".... CLEANING DATA .... REMOVING OUTLIERS ....\n",
      ".... TRAINING MODEL 1 ....\n",
      "\n",
      "\n",
      ".... GETTING RESULTS FOR MODEL 1....\n",
      "ADJUSTED R2 =  0.765818282025\n",
      "\n",
      "\n",
      "Model Coefficients:\n",
      "[  8.88925118e-03   2.56322289e+02]\n"
     ]
    }
   ],
   "source": [
    "print \"*\" * 30\n",
    "print \"MODEL 1 - Using all 27 properties for modeling impressions\"\n",
    "print \".... CLEANING DATA .... REMOVING OUTLIERS ....\"\n",
    "linModel1 = Model()\n",
    "linModel1.clean(quantile = .1)\n",
    "\n",
    "print \".... TRAINING MODEL 1 ....\"\n",
    "linModel1.train(model = Ridge, perc_train = .9, alpha = .1)\n",
    "\n",
    "print \".... GETTING RESULTS FOR MODEL 1....\"\n",
    "score = linModel1.r_score\n",
    "print \"ADJUSTED R2 = \", score\n",
    "\n",
    "results = linModel1.get_results()\n",
    "print \"Model Coefficients:\"\n",
    "coefs = linModel1.get_coefs()\n",
    "print coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 1 RESULTS:\n",
      "     page_id      tw_name  frequency       err_sum   err_mean\n",
      "0     759251          CNN        350   4086.403460  11.675438\n",
      "1   19426551          nfl        123    875.916783   7.121275\n",
      "2    2367911          MTV       5697  17919.794619   3.145479\n",
      "3   30309979   106andpark        316    585.683421   1.853429\n",
      "4   16560657          bet       7939   8510.282256   1.071959\n",
      "5   14934818      InStyle      14974  13662.544602   0.912418\n",
      "6   32448740    brueggers         66     55.459232   0.840291\n",
      "7  634784951  dasaniwater          1      0.814984   0.814984\n",
      "8   27677483   essencemag       5839   4565.960017   0.781976\n",
      "9   18342955   abc11_wtvd       4178   3119.168456   0.746570\n",
      "\n",
      "BIGGEST ERRORS:\n",
      "     page_id      tw_name\n",
      "0     759251          CNN\n",
      "1   19426551          nfl\n",
      "2    2367911          MTV\n",
      "3   30309979   106andpark\n",
      "4   16560657          bet\n",
      "5   14934818      InStyle\n",
      "6   32448740    brueggers\n",
      "7  634784951  dasaniwater\n",
      "8   27677483   essencemag\n",
      "9   18342955   abc11_wtvd\n"
     ]
    }
   ],
   "source": [
    "err_threshold = 0.6\n",
    "model_err_stats = linModel1.model_results.groupby(['page_id','tw_name'])['perc_diff'].agg(['count','sum','mean']).sort_values('mean',ascending = False, axis =0).reset_index()\n",
    "model_err_stats.rename(columns = {'count':'frequency', 'sum':'err_sum' ,'mean':'err_mean'}, inplace = True)\n",
    "tw_names_drop = model_err_stats[model_err_stats.err_mean > err_threshold][['page_id','tw_name']].drop_duplicates()\n",
    "\n",
    "print \"MODEL 1 RESULTS:\"\n",
    "print model_err_stats.head(10)\n",
    "print\n",
    "print \"BIGGEST ERRORS:\"\n",
    "print tw_names_drop.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...5-fold cross validation... \n",
      "[fold 0] alpha: 0.000100000, score: 0.76615\n",
      "[fold 1] alpha: 0.000152831, score: 0.76248\n",
      "[fold 2] alpha: 0.000233572, score: 0.76689\n",
      "[fold 3] alpha: 0.000356970, score: 0.75942\n",
      "[fold 4] alpha: 0.000545559, score: 0.76597\n",
      "Best alpha's\n",
      "      alpha  fold     score\n",
      "2  0.000234     2  0.766890\n",
      "0  0.000100     0  0.766153\n",
      "4  0.000546     4  0.765967\n",
      "1  0.000153     1  0.762475\n",
      "3  0.000357     3  0.759420\n",
      "\n",
      "FOUND BEST ALPHA TO USE IN MODEL 1\n",
      ".... TRAINING MODEL 1 with alpha = ... 0.000233572146909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:54: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ".... GETTING RESULTS FOR MODEL 1....\n",
      "ADJUSTED R2 =  0.762183871331\n",
      "\n",
      "\n",
      "Model Coefficients:\n",
      "[  8.94001327e-03   2.54000698e+02]\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross validation and search for best alpha\n",
    "print \"...5-fold cross validation... \"\n",
    "alphas = np.logspace(-4, -.5, 20)\n",
    "best_alpha = linModel1.cross_validate(alphas=alphas, folds=5)\n",
    "print \"\\nFOUND BEST ALPHA TO USE IN MODEL 1\"\n",
    "print \".... TRAINING MODEL 1 with alpha = ...\", best_alpha\n",
    "linModel1 = Model()\n",
    "linModel1.clean(quantile = .1)\n",
    "linModel1.train(model = Ridge, perc_train = .9, alpha = best_alpha)\n",
    "print \"\\n\"\n",
    "print \".... GETTING RESULTS FOR MODEL 1....\"\n",
    "score = linModel1.r_score\n",
    "print \"ADJUSTED R2 = \", score\n",
    "print \"\\n\"\n",
    "results = linModel1.get_results()\n",
    "print \"Model Coefficients:\"\n",
    "coefs = linModel1.get_coefs()\n",
    "print coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 1 RESULTS USING BEST ALPHA:\n",
      "     page_id      tw_name  frequency       err_sum   err_mean\n",
      "0     759251          CNN        350   4086.403460  11.675438\n",
      "1   19426551          nfl        123    875.916783   7.121275\n",
      "2    2367911          MTV       5697  17919.794619   3.145479\n",
      "3   30309979   106andpark        316    585.683421   1.853429\n",
      "4   16560657          bet       7939   8510.282256   1.071959\n",
      "5   14934818      InStyle      14974  13662.544602   0.912418\n",
      "6   32448740    brueggers         66     55.459232   0.840291\n",
      "7  634784951  dasaniwater          1      0.814984   0.814984\n",
      "8   27677483   essencemag       5839   4565.960017   0.781976\n",
      "9   18342955   abc11_wtvd       4178   3119.168456   0.746570\n",
      "\n",
      "BIGGEST ERRORS:\n",
      "     page_id      tw_name\n",
      "0     759251          CNN\n",
      "1   19426551          nfl\n",
      "2    2367911          MTV\n",
      "3   30309979   106andpark\n",
      "4   16560657          bet\n",
      "5   14934818      InStyle\n",
      "6   32448740    brueggers\n",
      "7  634784951  dasaniwater\n",
      "8   27677483   essencemag\n",
      "9   18342955   abc11_wtvd\n"
     ]
    }
   ],
   "source": [
    "err_threshold = 0.6\n",
    "model_err_stats = linModel1.model_results.groupby(['page_id','tw_name'])['perc_diff'].agg(['count','sum','mean']).sort_values('mean',ascending = False, axis =0).reset_index()\n",
    "model_err_stats.rename(columns = {'count':'frequency', 'sum':'err_sum' ,'mean':'err_mean'}, inplace = True)\n",
    "tw_names_drop = model_err_stats[model_err_stats.err_mean > err_threshold][['page_id','tw_name']].drop_duplicates()\n",
    "\n",
    "print \"MODEL 1 RESULTS USING BEST ALPHA:\"\n",
    "print model_err_stats.head(10)\n",
    "print\n",
    "print \"BIGGEST ERRORS:\"\n",
    "print tw_names_drop.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression - Model 2  \n",
    "- remove properties with largest errors (over 60% incorrect on average)\n",
    "- use 5-fold cross-validation for best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "MODEL 2\n",
      "REMOVING TWITTER USERS WITH AVG ERRORS > 60%\n",
      ".... CLEANING DATA .... REMOVING OUTLIERS ....\n",
      "\n",
      "\n",
      "5 FOLD CROSS VALIDATION ....\n",
      "[fold 0] alpha: 0.000100000, score: 0.81777\n",
      "[fold 1] alpha: 0.000152831, score: 0.82348\n",
      "[fold 2] alpha: 0.000233572, score: 0.82171\n",
      "[fold 3] alpha: 0.000356970, score: 0.81639\n",
      "[fold 4] alpha: 0.000545559, score: 0.81868\n",
      "Best alpha's\n",
      "      alpha  fold     score\n",
      "1  0.000153     1  0.823482\n",
      "2  0.000234     2  0.821708\n",
      "4  0.000546     4  0.818679\n",
      "0  0.000100     0  0.817768\n",
      "3  0.000357     3  0.816386\n",
      "\n",
      "FOUND BEST ALPHA USED IN MODEL 2:  0.000152830673266\n",
      ".... TRAINING MODEL 2....\n",
      "ADJUSTED R2 =  0.816688528879\n",
      "Model Coefficients:\n",
      "[  8.76712081e-03   2.40737750e+02]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-4, -.5, 20)\n",
    "print \"*\" * 30\n",
    "print \"MODEL 2\"\n",
    "print \"REMOVING TWITTER USERS WITH AVG ERRORS > 60%\"\n",
    "\n",
    "\n",
    "linModel2 = Model()\n",
    "# remove names with largest average of error\n",
    "linModel2.tw_data = linModel2.tw_data[~linModel2.tw_data.page_id.isin(tw_names_drop.page_id)]\n",
    "print \".... CLEANING DATA .... REMOVING OUTLIERS ....\"\n",
    "linModel2.clean(quantile = .1)\n",
    "print \"\\n\"\n",
    "print \"5 FOLD CROSS VALIDATION ....\"\n",
    "best_alpha = linModel2.cross_validate(alphas=alphas, folds=5)\n",
    "print\n",
    "print \"FOUND BEST ALPHA USED IN MODEL 2: \", best_alpha\n",
    "print \".... TRAINING MODEL 2....\"\n",
    "linModel2.train(model = Ridge, perc_train = .9, alpha = best_alpha)\n",
    "score2 = linModel2.r_score\n",
    "print \"ADJUSTED R2 = \", score2\n",
    "results2 = linModel2.get_results()\n",
    "print \"Model Coefficients:\"\n",
    "coefs2 = linModel2.get_coefs()\n",
    "print coefs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 2 RESULTS USING BEST ALPHA:\n",
      "      page_id          tw_name  frequency       err_sum  err_mean\n",
      "0   192981351     LandRoverUSA         42     21.147072  0.503502\n",
      "1    25053299  fortunemagazine      22297  10810.510498  0.484841\n",
      "2    21308602   cartoonnetwork        247    108.331923  0.438591\n",
      "3     5988062     theeconomist       3055   1167.208866  0.382065\n",
      "4    14946736          DIRECTV        307    110.054806  0.358485\n",
      "5    73200694            Coach        122     36.610598  0.300087\n",
      "6   436171805        fusionpop         25      7.497409  0.299896\n",
      "7   226299107          betnews         29      7.099350  0.244805\n",
      "8     9695312        billboard      11431   2498.741824  0.218593\n",
      "9   119606058   aquiyahorashow         14      2.841013  0.202930\n",
      "10   16374678             ABC7      11804   2360.990145  0.200016\n",
      "11   25589776           people      25607   3735.139277  0.145864\n",
      "12   14293310             TIME      29090   3846.456865  0.132226\n",
      "\n",
      "BIGGEST ERRORS:\n",
      "Empty DataFrame\n",
      "Columns: [page_id, tw_name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "model_err_stats2 = linModel2.model_results.groupby(['page_id','tw_name'])['perc_diff'].agg(['count','sum','mean']).sort_values('mean',ascending = False, axis =0).reset_index()\n",
    "model_err_stats2.rename(columns = {'count':'frequency', 'sum':'err_sum' ,'mean':'err_mean'}, inplace = True)\n",
    "tw_names_drop2 = model_err_stats2[model_err_stats2.err_mean > err_threshold][['page_id','tw_name']].drop_duplicates()\n",
    "\n",
    "print \"MODEL 2 RESULTS USING BEST ALPHA:\"\n",
    "print model_err_stats2\n",
    "print\n",
    "print \"BIGGEST ERRORS:\"\n",
    "print tw_names_drop2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression - Model 3  \n",
    "- model using properties where error is >60%\n",
    "- perform 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "MODEL 3\n",
      "******************************\n",
      "BUILDING MODEL FOR TWITTER USERS GUILTY OF LARGEST ERRORS\n",
      ".... CLEANING DATA .... REMOVING OUTLIERS ....\n",
      "\n",
      "\n",
      "10 FOLD CROSS VALIDATION ....\n",
      "[fold 0] alpha: 0.000100000, score: 0.89105\n",
      "[fold 1] alpha: 0.000152831, score: 0.89236\n",
      "[fold 2] alpha: 0.000233572, score: 0.88658\n",
      "[fold 3] alpha: 0.000356970, score: 0.89746\n",
      "[fold 4] alpha: 0.000545559, score: 0.88830\n",
      "[fold 5] alpha: 0.000833782, score: 0.89534\n",
      "[fold 6] alpha: 0.001274275, score: 0.88810\n",
      "[fold 7] alpha: 0.001947483, score: 0.89164\n",
      "[fold 8] alpha: 0.002976351, score: 0.89149\n",
      "[fold 9] alpha: 0.004548778, score: 0.89680\n",
      "Best alpha's\n",
      "      alpha  fold     score\n",
      "3  0.000357     3  0.897459\n",
      "9  0.004549     9  0.896798\n",
      "5  0.000834     5  0.895344\n",
      "1  0.000153     1  0.892364\n",
      "7  0.001947     7  0.891644\n",
      "8  0.002976     8  0.891486\n",
      "0  0.000100     0  0.891055\n",
      "4  0.000546     4  0.888304\n",
      "6  0.001274     6  0.888100\n",
      "2  0.000234     2  0.886581\n",
      "\n",
      "FOUND BEST ALPHA USED IN MODEL 3:  0.000356969884683\n",
      ".... TRAINING MODEL 3....\n",
      "ADJUSTED R2 =  0.888740204144\n",
      "Model Coefficients:\n",
      "[  1.24300940e-02   1.52247788e+02]\n"
     ]
    }
   ],
   "source": [
    "print \"*\" * 30\n",
    "print \"MODEL 3\"\n",
    "\n",
    "print \"*\" * 30\n",
    "print \"BUILDING MODEL FOR TWITTER USERS GUILTY OF LARGEST ERRORS\"\n",
    "\n",
    "linModel3 = Model()\n",
    "linModel3.tw_data = linModel3.tw_data[linModel3.tw_data.page_id.isin(tw_names_drop.page_id)]\n",
    "print \".... CLEANING DATA .... REMOVING OUTLIERS ....\"\n",
    "linModel3.clean(quantile = .1)\n",
    "print \"\\n\"\n",
    "print \"10 FOLD CROSS VALIDATION ....\"\n",
    "best_alpha_3 = linModel3.cross_validate(alphas=alphas, folds=10)\n",
    "print\n",
    "print \"FOUND BEST ALPHA USED IN MODEL 3: \", best_alpha_3\n",
    "print \".... TRAINING MODEL 3....\"\n",
    "linModel3.train(model = Ridge, perc_train = .9, alpha = best_alpha_3)\n",
    "score3 = linModel3.r_score\n",
    "print \"ADJUSTED R2 = \", score3\n",
    "print \"Model Coefficients:\"\n",
    "coefs3 = linModel3.get_coefs()\n",
    "print coefs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 3 RESULTS USING BEST ALPHA:\n",
      "       page_id          tw_name  frequency       err_sum  err_mean\n",
      "0     18342955       abc11_wtvd       3294  12386.628306  3.760361\n",
      "1     32448740        brueggers         48    165.043697  3.438410\n",
      "2   1426645165           bustle       3209  10950.713596  3.412500\n",
      "3    223525053     ringlingbros         30     95.801266  3.193376\n",
      "4      2367911              MTV       6596  19620.996732  2.974681\n",
      "5     27677483       essencemag       5628  15410.318480  2.738152\n",
      "6     25453312  hallmarkchannel       2481   6604.009992  2.661834\n",
      "7     19426551              nfl        644   1702.890740  2.644240\n",
      "8     30309979       106andpark        339    651.353689  1.921397\n",
      "9    634784951      dasaniwater          2      2.129312  1.064656\n",
      "10      759251              CNN      13272   8216.304356  0.619071\n",
      "11    14934818          InStyle      15052   8632.908271  0.573539\n",
      "12    16560657              bet       8277   1810.044625  0.218684\n"
     ]
    }
   ],
   "source": [
    "model_err_stats3 = linModel3.model_results.groupby(['page_id','tw_name'])['perc_diff'].agg(['count','sum','mean']).sort_values('mean',ascending = False, axis =0).reset_index()\n",
    "model_err_stats3.rename(columns = {'count':'frequency', 'sum':'err_sum' ,'mean':'err_mean'}, inplace = True)\n",
    "\n",
    "print \"MODEL 3 RESULTS USING BEST ALPHA:\"\n",
    "print model_err_stats3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
